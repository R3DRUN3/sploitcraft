import re
from fastapi import FastAPI
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import os
import subprocess
from dotenv import load_dotenv
from kubernetes import client, config
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from typing import List

# Load environment variables
load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')

# Detect environment
if os.getenv('KUBERNETES_SERVICE_HOST'):
    config.load_incluster_config()
else:
    config.load_kube_config()

# Initialize Kubernetes API client
k8s_api = client.CoreV1Api()

# Initialize ChatOpenAI model
chat = ChatOpenAI(model="gpt-4", openai_api_key=openai_api_key)

# Define the prompt template (Chatbot can execute Kubernetes commands)
prompt = ChatPromptTemplate.from_messages([
    ("system", """
    You are Opsie, a friendly DevOps AI assistant running on a kubernetes cluster. 
    You can run Kubernetes commands on behalf of users. 
    For example, if a user ask for the list of all pods, you should produce the "kubectl get pods -A" command and so on.   
    """),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# In-memory chat message history
class InMemoryHistory(BaseChatMessageHistory, BaseModel):
    messages: List[BaseMessage] = []

    def add_messages(self, messages: List[BaseMessage]) -> None:
        self.messages.extend(messages)

    def clear(self) -> None:
        self.messages = []

# Store chat histories
store = {}

def get_by_session_id(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = InMemoryHistory()
    return store[session_id]

# Create conversation chain
conversation = RunnableWithMessageHistory(
    runnable=prompt | chat,
    get_session_history=get_by_session_id,
    input_messages_key="input",
    history_messages_key="history"
)

# FastAPI app
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Chat API Endpoint
class ChatRequest(BaseModel):
    input: str
    session_id: str

class ChatResponse(BaseModel):
    content: str


@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    session_id = request.session_id
    user_input = request.input.strip()

    response = conversation.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": session_id}}
    )

    ai_response = response.content

    command_pattern = r"```(?:bash)?\s*(kubectl [^`]+)\s*```"
    match = re.search(command_pattern, ai_response, re.IGNORECASE)

    if match:
        command_to_execute = match.group(1).strip()
        try:
            output = subprocess.check_output(
                command_to_execute, shell=True, stderr=subprocess.STDOUT, timeout=10
            ).decode()

            # Convert command output to HTML table
            lines = output.strip().split("\n")
            headers = lines[0].split()
            rows = [line.split(None, len(headers)-1) for line in lines[1:]]

            table_html = "<table border='1' style='border-collapse: collapse; width: 100%;'>"
            table_html += "<thead><tr>" + "".join(f"<th>{h}</th>" for h in headers) + "</tr></thead><tbody>"
            for row in rows:
                table_html += "<tr>" + "".join(f"<td>{col}</td>" for col in row) + "</tr>"
            table_html += "</tbody></table>"

            final_response = (
                f"✅ <b>Executed Command:</b><br><code>{command_to_execute}</code><br><br>"
                f"<b>Command Output:</b><br>{table_html}"
            )

            return {"content": final_response}

        except Exception as e:
            return {"content": f"❌ Command execution failed:<br><pre>{str(e)}</pre>"}

    # Default response
    return {"content": ai_response.replace("\n", "<br>")}




# Serve static files (your frontend)
app.mount("/static", StaticFiles(directory="static"), name="static")

# Serve HTML frontend
@app.get("/")
async def serve_root():
    return FileResponse("static/index.html")
